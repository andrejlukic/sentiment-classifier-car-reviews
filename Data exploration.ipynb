{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neg', 'Pos'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/car-reviews.csv\")\n",
    "df.head()\n",
    "df[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andrejwork/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrejwork/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return word_tokenize(text)  \n",
    "    \n",
    "def remove_stop_words(word_list):\n",
    "    from nltk.corpus import stopwords     \n",
    "    stop_words = set(stopwords.words('english'))   \n",
    "    filtered = [w for w in word_list if not w in stop_words]  \n",
    "    return filtered\n",
    "\n",
    "def lower_case(word_list):\n",
    "    return [w.lower() for w in word_list]\n",
    "\n",
    "def stem_words(list_of_words):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = [ps.stem(w) for w in list_of_words]\n",
    "    return stemmed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', '1992', 'bought', 'new', 'Taurus', 'really', 'loved', 'So', '1999', 'decided', 'try', 'new', 'Taurus', 'I', 'care', 'style', 'newer', 'version', 'bought', 'anyway', 'I', 'like', 'new', 'car', 'half', 'much', 'liked', 'one', 'Thee', 'dash', 'much', 'deep', 'takes', 'lot', 'room', 'I', 'find', 'seats', 'comfortable', 'way', 'sides', 'stick', 'strip', 'protect', 'card', 'denting', 'It', 'drives', 'nice', 'good', 'pick', 'But', 'see', 'hood', 'driver', 'seat', 'judging', 'parking', 'difficult', 'It', 'small', 'gas', 'tank', 'I', 'would', 'buy', 'Taurus', 'I', 'I', 'would', 'rather', '1992', 'back', 'I', 'dont', 'think', 'style', 'nice', '1992', 'mistake', 'change', 'style', 'In', 'less', 'month', 'dead', 'battery', 'flat', 'tire']\n",
      "\n",
      "['in', '1992', 'bought', 'new', 'tauru', 'realli', 'love', 'so', '1999', 'decid', 'tri', 'new', 'tauru', 'i', 'care', 'style', 'newer', 'version', 'bought', 'anyway', 'i', 'like', 'new', 'car', 'half', 'much', 'like', 'one', 'thee', 'dash', 'much', 'deep', 'take', 'lot', 'room', 'i', 'find', 'seat', 'comfort', 'way', 'side', 'stick', 'strip', 'protect', 'card', 'dent', 'it', 'drive', 'nice', 'good', 'pick', 'but', 'see', 'hood', 'driver', 'seat', 'judg', 'park', 'difficult', 'it', 'small', 'ga', 'tank', 'i', 'would', 'buy', 'tauru', 'i', 'i', 'would', 'rather', '1992', 'back', 'i', 'dont', 'think', 'style', 'nice', '1992', 'mistak', 'chang', 'style', 'in', 'less', 'month', 'dead', 'batteri', 'flat', 'tire']\n"
     ]
    }
   ],
   "source": [
    "# Test clean up on one review\n",
    "tokenized = tokenize(df[\"Review\"][0])\n",
    "clean = remove_stop_words(tokenized)\n",
    "print(clean)\n",
    "print()\n",
    "lowered = lower_case(clean)\n",
    "stemmed = stem_words(lowered)\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that generates bag of words and turns list of\n",
    "def generate_bow(list_tokenized_reviews):\n",
    "    import numpy as np\n",
    "    \n",
    "    bow = {}\n",
    "    for r in list_tokenized_reviews:\n",
    "        for w in r:\n",
    "            if w in bow:\n",
    "                bow[w] += 1\n",
    "            else:\n",
    "                bow[w] = 1\n",
    "    \n",
    "    list_features = []\n",
    "    for review in list_tokenized_reviews:\n",
    "        vector = np.zeros(len(bow))\n",
    "        for index, word in enumerate(bow):\n",
    "            if word in review:\n",
    "                vector[index] += 1\n",
    "        list_features.append(vector)\n",
    "    return list_features, bow\n",
    "\n",
    "# Function that generates bag of words and turns list of\n",
    "def generate_bow_faster(list_tokenized_reviews):\n",
    "    # This is 6.5x faster than the previous version\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    bow = {}\n",
    "    for r in list_tokenized_reviews:\n",
    "        for w in r:\n",
    "            if w in bow:\n",
    "                bow[w] += 1\n",
    "            else:\n",
    "                bow[w] = 1\n",
    "    #print(\"BOW \",len(bow))\n",
    "    bow_ordered = OrderedDict(sorted(bow.items(), key=lambda t: t[0]))\n",
    "    inx = 0\n",
    "    for key in bow_ordered:\n",
    "        bow_ordered[key] = (inx, bow_ordered[key])\n",
    "        inx += 1\n",
    "    #print(\"BOW ORDERED \",len(bow_ordered))\n",
    "    #print(bow_ordered)\n",
    "    list_features = []\n",
    "    for review in list_tokenized_reviews:\n",
    "        vector = [0] * len(bow)\n",
    "        for i in range(len(review)):\n",
    "            inx = bow_ordered[review[i]][0]\n",
    "            #print(\"INX \",inx)\n",
    "            vector[inx] += 1\n",
    "        list_features.append(vector)\n",
    "    return list_features, bow_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 1., 1., 0., 0., 0.]), array([0., 1., 1., 1., 1., 1., 1.])]\n",
      "{'drive': 1, 'nice': 2, 'good': 2, 'pick': 2, 'mistak': 1, 'chang': 1, 'style': 1}\n"
     ]
    }
   ],
   "source": [
    "# test bag of words\n",
    "test_list = [['drive', 'nice', 'good', 'pick'], ['mistak', 'chang', 'style', 'nice', 'good', 'pick']]\n",
    "features, bow = generate_bow(test_list)\n",
    "print(features)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataframe into a list of features\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "trans = [remove_stop_words, lower_case, stem_words]\n",
    "features = []\n",
    "for r in df[\"Review\"].to_list():\n",
    "    v = tokenize(r)\n",
    "    c = remove_stop_words(v)\n",
    "    l = lower_case(c)\n",
    "    s = stem_words(l)\n",
    "    features.append(s)\n",
    "\n",
    "features = generate_bow_faster(features)   \n",
    "t2 = time.time()\n",
    "print(\"Done in {:0.2f}sec\".format(t2-t1))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
